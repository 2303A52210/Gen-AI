{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOXpG1ZxMAoB9xlLtaxoFeG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2303A52210/Gen-AI/blob/main/week_1_ass1_2210.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Write Python code from scratch to find error metrics of deep learning model. Actual values and deep learning model predicted values are shown in Table 1. Also compare the results with the outcomes of libraries YActual YP red 20 20.5 30 30.3 40 40.2 50 50.6 60 60.7 Tabela 1: YActual Vs. YP red\n",
        "\n",
        "\n",
        "[ ]\n"
      ],
      "metadata": {
        "id": "GCH7gzSsEvAR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_actual = [20, 30, 40, 50, 60]\n",
        "y_pred = [20.5, 30.3, 40.2, 50.6, 60.7]\n",
        "\n",
        "sum_absolute_error = 0\n",
        "sum_squared_error = 0\n",
        "\n",
        "for actual, pred in zip(y_actual, y_pred):\n",
        "    error = actual - pred\n",
        "    sum_absolute_error += abs(error)\n",
        "    sum_squared_error += error ** 2\n",
        "\n",
        "n = len(y_actual)\n",
        "mae = sum_absolute_error / n\n",
        "mse = sum_squared_error / n\n",
        "rmse = mse ** 0.5\n",
        "\n",
        "print(f\"MAE: {mae}\")\n",
        "print(f\"MSE: {mse}\")\n",
        "print(f\"RMSE: {rmse}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3q_zIqa5ExyQ",
        "outputId": "a4d8ada3-27fb-4958-d2b6-cf53aefd1692"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE: 0.4600000000000016\n",
            "MSE: 0.24600000000000147\n",
            "RMSE: 0.49598387070549127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) Write python code from scratch to find evaluation metrics of deep learning model.\n"
      ],
      "metadata": {
        "id": "-F0k2jFrE1MJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_metrics(y_actual, y_pred):\n",
        "    metrics = {}\n",
        "\n",
        "    tp = sum(1 for a, p in zip(y_actual, y_pred) if a == p and p == 1)\n",
        "    tn = sum(1 for a, p in zip(y_actual, y_pred) if a == p and p == 0)\n",
        "    fp = sum(1 for a, p in zip(y_actual, y_pred) if a != p and p == 1)\n",
        "    fn = sum(1 for a, p in zip(y_actual, y_pred) if a != p and p == 0)\n",
        "\n",
        "\n",
        "    metrics['accuracy'] = (tp + tn) / len(y_actual)\n",
        "\n",
        "\n",
        "    metrics['precision'] = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "\n",
        "\n",
        "    metrics['recall'] = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "\n",
        "\n",
        "    precision = metrics['precision']\n",
        "    recall = metrics['recall']\n",
        "    metrics['f1_score'] = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "    return metrics\n",
        "\n",
        "\n",
        "y_actual = [0, 0, 1, 1, 2, 0, 0, 0, 1, 0, 2, 0, 0, 1, 1, 2, 2, 1, 0, 2, 1, 0, 2, 2, 0, 2, 1, 2, 2, 2]\n",
        "y_pred = [0, 0, 1, 0, 2, 0, 0, 1, 1, 2, 2, 1, 0, 2, 1, 0, 2, 2, 0, 2, 1, 2, 2, 2, 0, 2, 1, 2, 2, 2]\n",
        "\n",
        "scratch_metrics = calculate_metrics(y_actual, y_pred)\n",
        "print(\"Metrics calculated from scratch:\")\n",
        "for metric, value in scratch_metrics.items():\n",
        "    print(f\"{metric}: {value:.2f}\")\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "accuracy = accuracy_score(y_actual, y_pred)\n",
        "precision = precision_score(y_actual, y_pred, average='macro')  # Macro average for multi-class\n",
        "recall = recall_score(y_actual, y_pred, average='macro')\n",
        "f1 = f1_score(y_actual, y_pred, average='macro')\n",
        "\n",
        "print(\"\\nMetrics calculated using sklearn:\")\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1 Score: {f1:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1WjzKnsNE8T5",
        "outputId": "d262f93c-499e-4a6f-911d-35d79a3388ff"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metrics calculated from scratch:\n",
            "accuracy: 0.40\n",
            "precision: 0.71\n",
            "recall: 0.71\n",
            "f1_score: 0.71\n",
            "\n",
            "Metrics calculated using sklearn:\n",
            "Accuracy: 0.73\n",
            "Precision: 0.74\n",
            "Recall: 0.72\n",
            "F1 Score: 0.72\n"
          ]
        }
      ]
    }
  ]
}