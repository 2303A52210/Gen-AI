{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMKtL77kk3tnL5VPIB8GlyQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2303A52210/Gen-AI/blob/main/week_3_ass_3_2210.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Write Python code without using any libraries to find the value of x at which the function f(x) shown in equation (1) has minimum value. Consider Gradient Descent Algorithm. f (x) = 5x^4 + 3x^2 + 10\n",
        "\n"
      ],
      "metadata": {
        "id": "PAjmB87HGYeR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent_1d(alpha=0.01, epsilon=1e-6, max_iter=10000):\n",
        "    def f_prime(x):\n",
        "        return 20 * x**3 + 6 * x\n",
        "\n",
        "    x = 0.0\n",
        "    for _ in range(max_iter):\n",
        "        grad = f_prime(x)\n",
        "        if abs(grad) < epsilon:\n",
        "            break\n",
        "        x -= alpha * grad\n",
        "    return x\n",
        "\n",
        "x_min = gradient_descent_1d()\n",
        "print(f\"Minimum value of f(x) occurs at x = {x_min}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYOvtB3XGbtB",
        "outputId": "c72a1ddf-36e3-40da-8cde-537c78b22a75"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Minimum value of f(x) occurs at x = 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) Write Python code without using any libraries to find the value of x and y at which the function g(x,y) shown in equation (2) has minimum value. Consider Gradient Descent Algorithm. f (x) = 3x^2 + 5e^−y + 10\n",
        "\n"
      ],
      "metadata": {
        "id": "h6iIr9ONGeOA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent_2d(alpha=0.01, epsilon=1e-6, max_iter=10000):\n",
        "    def f_prime_x(x):\n",
        "        return 6 * x\n",
        "\n",
        "    def f_prime_y(y):\n",
        "        return -5 * (2.718281828459045 ** (-y))\n",
        "\n",
        "    x, y = 0.0, 0.0\n",
        "    for _ in range(max_iter):\n",
        "        grad_x = f_prime_x(x)\n",
        "        grad_y = f_prime_y(y)\n",
        "        if abs(grad_x) < epsilon and abs(grad_y) < epsilon:\n",
        "            break\n",
        "        x -= alpha * grad_x\n",
        "        y -= alpha * grad_y\n",
        "    return x, y\n",
        "\n",
        "x_min, y_min = gradient_descent_2d()\n",
        "print(f\"Minimum value of g(x, y) occurs at x = {x_min}, y = {y_min}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JyNYnwzMGgk_",
        "outputId": "27dd05e8-e436-4350-b806-34b277f23223"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Minimum value of g(x, y) occurs at x = 0.0, y = 6.216917124174048\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) Write Python code without using any libraries to find the value of x at which the sigmoid function z(x) shown in equation (3) has minimum value. Consider Gradient Descent Algorithm. z(x) = 1/1 + e−x (3)"
      ],
      "metadata": {
        "id": "Bw5duVvUGkZh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent_sigmoid(alpha=0.01, epsilon=1e-6, max_iter=10000):\n",
        "    def z_prime(x):\n",
        "        exp_neg_x = 2.718281828459045 ** (-x)\n",
        "        return -exp_neg_x / ((1 + exp_neg_x) ** 2)\n",
        "\n",
        "    x = 0.0  # Initial guess\n",
        "    for _ in range(max_iter):\n",
        "        grad = z_prime(x)\n",
        "        if abs(grad) < epsilon:\n",
        "            break\n",
        "        x -= alpha * grad\n",
        "    return x\n",
        "\n",
        "x_min_sigmoid = gradient_descent_sigmoid()\n",
        "print(f\"Minimum value of z(x) occurs at x = {x_min_sigmoid}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vv4Bn12gGnqZ",
        "outputId": "10b11297-c166-4647-83f2-63cff46b3c1e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Minimum value of z(x) occurs at x = 4.510913300793877\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4) Write Python code without using any libraries to find the value of optimal values of model parameters M and C such that the model’s Square Error Value shown in equation 4 will be minimum. It means model gives output close to expected output"
      ],
      "metadata": {
        "id": "8H0S42uSGqUZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent_linear(alpha=0.01, epsilon=1e-6, max_iter=10000):\n",
        "    expected_outputs = [1, 2, 3, 4, 5]\n",
        "    inputs = [1, 2, 3, 4, 5]\n",
        "\n",
        "    def predicted_output(m, c, x):\n",
        "        return m * x + c\n",
        "\n",
        "    def se_grad_m(m, c):\n",
        "        return -2 * sum((e - predicted_output(m, c, x)) * x for e, x in zip(expected_outputs, inputs))\n",
        "\n",
        "    def se_grad_c(m, c):\n",
        "        return -2 * sum(e - predicted_output(m, c, x) for e, x in zip(expected_outputs, inputs))\n",
        "\n",
        "    m, c = 0.0, 0.0\n",
        "    for _ in range(max_iter):\n",
        "        grad_m = se_grad_m(m, c)\n",
        "        grad_c = se_grad_c(m, c)\n",
        "        if abs(grad_m) < epsilon and abs(grad_c) < epsilon:\n",
        "            break\n",
        "        m -= alpha * grad_m\n",
        "        c -= alpha * grad_c\n",
        "    return m, c\n",
        "\n",
        "m_opt, c_opt = gradient_descent_linear()\n",
        "print(f\"Optimal values are M = {m_opt}, C = {c_opt}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBobIJh3G1-5",
        "outputId": "7c56c5fe-443f-4a7b-8e77-81186a5f5e20"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal values are M = 0.9999998375829822, C = 5.863769691679814e-07\n"
          ]
        }
      ]
    }
  ]
}